{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQlC7GK-knm3"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKx95C2CkZr9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizers\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import statistics\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cgAW4Rpk9va"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Jrmg3dydJT"
      },
      "source": [
        "## Pixel Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpxb7eHAyiT6"
      },
      "outputs": [],
      "source": [
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PixelNorm, self).__init__()\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_4y43YC8EkI"
      },
      "source": [
        "## Weight-Scaled Convolution (Equalized Learning Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8wMrpD58SD-"
      },
      "outputs": [],
      "source": [
        "class WSConv2d(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=\"same\", gain=2, conv = True):\n",
        "        super(WSConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = padding) if conv else nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
        "        # only weights should be scaled and not the bias terms, thus the lines below\n",
        "        self.bias = self.conv.bias\n",
        "        self.conv.bias = None\n",
        "\n",
        "        # initialize conv layer\n",
        "        nn.init.normal_(self.conv.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl1iwXfblFJB"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0zAXWG1k8_D"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_pixelnorm=True, downscale = True):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.use_pn = use_pixelnorm\n",
        "        self.use_downscaling = downscale\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2)  \n",
        "        self.pn = PixelNorm()\n",
        "        self.downscaled = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.leaky(self.pn(self.conv1(x))) if self.use_pn else self.leaky(self.conv1(x))\n",
        "        x2 = self.pn(self.conv2(x1)) if self.use_pn else self.conv2(x1)\n",
        "        x3 = self.leaky(x2 + x1) #skip connection\n",
        "        out = self.downscaled(x3) if self.use_downscaling else x3\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJfPp4n7lLK1"
      },
      "source": [
        "## Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mQNSby9lnJ1"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_pixelnorm=True, upscale=True):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.use_pn = use_pixelnorm\n",
        "        self.use_upscaling = upscale\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "        self.pn = PixelNorm()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upscaled(x) if self.use_upscaling else x\n",
        "        x1 = self.leaky(self.pn(self.conv1(x))) if self.use_pn else self.leaky(self.conv1(x))\n",
        "        x2 = self.pn(self.conv2(x1)) if self.use_pn else self.conv2(x1)\n",
        "        out = self.leaky(x2 + x1) #skip connection\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWT7I3C3lTAV"
      },
      "source": [
        "## U-Net Generator (Autoencoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx6ErXa-k6Is"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.WSConv2d(in_channels, features, kernel_size = 4, stride = 2, padding = 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        ) #out = (128*128,64)\n",
        "        \n",
        "        self.enc2 = EncoderBlock(features, features * 2) #out = (64*64,128)\n",
        "\n",
        "        self.enc3 = EncoderBlock(features * 2, features * 4) #out = (32*32,256)\n",
        "\n",
        "        self.enc4 = EncoderBlock(features * 4, features * 8) #out = (16*16,512)\n",
        "\n",
        "        self.enc5 = EncoderBlock(features * 8, features * 8) #out = (8*8,512)\n",
        "\n",
        "        self.enc6 = EncoderBlock(features * 8, features * 8) #out = (4*4,512)\n",
        "\n",
        "        self.enc7 = EncoderBlock(features * 8, features * 8) #out = (2*2,512)\n",
        "\n",
        "        self.enc8 = EncoderBlock(features * 8, features * 8) #out = (1*1,512)\n",
        "\n",
        "        # for decoder blocks [dec2:] the number of input channels have been doubled to incorporate the skip connections (U-Net)\n",
        "\n",
        "        self.dec1 = DecoderBlock(features * 8 * 2, features * 8) #out = (2*2,512)\n",
        "\n",
        "        self.dec2 = DecoderBlock(features * 8 * 2, features * 8) #out = (4*4,512)\n",
        "\n",
        "        self.dec3 = DecoderBlock(features * 8 * 2, features * 8) #out = (8*8,512)\n",
        "\n",
        "        self.dec4 = DecoderBlock(features * 8 * 2, features * 8) #out = (16*16,512)\n",
        "\n",
        "        self.dec5 = DecoderBlock(features * 8 * 2, features * 4) #out = (32*32,256)\n",
        "\n",
        "        self.dec6 = DecoderBlock(features * 4 * 2, features * 2) #out = (64*64,128)\n",
        "\n",
        "        self.dec7 = DecoderBlock(features * 2 * 2, features) #out = (128*128, 64)\n",
        "\n",
        "        self.dec8 = nn.Sequential(\n",
        "            WSConv2d(features * 2, in_channels, kernel_size = 4, stride = 2, padding = 1, conv = False), #transpose convolution\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "        e6 = self.enc6(e5)\n",
        "        e7 = self.enc7(e6)\n",
        "\n",
        "        bottleneck = self.enc8(e7)\n",
        "\n",
        "        d1 = self.dec1(bottleneck)\n",
        "        d2 = self.dec2(torch.cat([d1, e7], 1))\n",
        "        d3 = self.dec3(torch.cat([d2, e6], 1))\n",
        "        d4 = self.dec4(torch.cat([d3, e5], 1))\n",
        "        d5 = self.dec5(torch.cat([d4, e4], 1))\n",
        "        d6 = self.dec6(torch.cat([d5, e3], 1))\n",
        "        d7 = self.dec7(torch.cat([d6, e2], 1))\n",
        "        \n",
        "        return self.dec8(torch.cat([[d7, e1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsN9MR0blqWc"
      },
      "source": [
        "# Discriminator (PatchGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw--XlpFlzf2"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels * 2,\n",
        "                features[0],\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                padding_mode=\"reflect\",\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(\n",
        "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        x = self.initial(x)\n",
        "        x = self.model(x)\n",
        "        return x ## 26*26 matrix"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
